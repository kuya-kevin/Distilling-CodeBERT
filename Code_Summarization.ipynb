{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Code Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bO4r-iI8fgm"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In recent time, large, pre-trained, language models had shown high potential is several tasks. Such as Question Answering, Sentiment Analysis, Abstractive Summary etc. Some of the important models include [ELMo (Peters\n",
        "et al., 2018)](https://arxiv.org/abs/1802.05365), [GPT (Radford et al., 2018)](https://arxiv.org/abs/2005.14165), [BERT (Devlin et al., 2018)](https://arxiv.org/pdf/1810.04805.pdf), [XLNet (Yang et al., 2019)](https://arxiv.org/pdf/1906.08237.pdf), and [RoBERTa (Liu et al., 2019)](https://arxiv.org/pdf/1907.11692.pdf).\n",
        "\n",
        "They all follow the base architecture proposed by Vaswani et. al. in their Seminal Paper: [Attention is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "Following the The naturalness hypothesis of source code, proposed by Allamanis et. al. it is thus preferable to try to treat large code corpora in the similar fashion and exploit their Statistical Properties. \n",
        "\n",
        "In this colab, we will see an end to end pipeline, using [Hugging Face Transformers Library](https://huggingface.co/transformers/), [Microsoft's Open Source Large Scale code model CodeBERT](https://huggingface.co/microsoft/codebert-base), and [Codist tree-hugger](https://github.com/autosoft-dev/tree-hugger) how to use similar technology to a very challenging problem called Code Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV_yl7YGrb99"
      },
      "source": [
        "## Background\n",
        "\n",
        "![transformer](https://lilianweng.github.io/lil-log/assets/images/transformer.png)\n",
        "\n",
        "<h3 align=\"center\">The full model architecture of the transformer. (Image source: Fig 1 & 2 in Vaswani, et al., 2017.)</h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSdr5w_svmLp"
      },
      "source": [
        "We won't go in detail of the transformer architecure. Because that is not really in the scope of this tutorial and there are plenty of very good resources that do justice to it. Please check out [here](https://jalammar.github.io/illustrated-transformer/), [here](http://nlp.seas.harvard.edu/2018/04/03/attention.html), and [here](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html). \n",
        "\n",
        "For this particular tutorial we are using Microsoft CodeBERT as the baseline model and this model has been proposed in a [paper](https://arxiv.org/pdf/2002.08155.pdf) in 2020 by Zhangyin Feng et. al. It is [freely available](https://huggingface.co/microsoft/codebert-base) via Hugging Face model repository. \n",
        "\n",
        "We at [Codist](https://codist-ai.com/) has released a new model and a tool called `docly` that does exactly same work (and a lot more!). We found our model slightly outperforming MS CodeBERT in some cases. If you are interested in it, please go to the website and sign up for a beta.\n",
        "\n",
        "MS CodeBERT has been trained on a hybrid objective function where the model was simultaneously predicting both masked tokens and replaced tokens. So the final loss of the model can be expressed by\n",
        "\n",
        "$$\n",
        "min_\\theta = L_{MLM}(\\theta) + L_{RTD}(\\theta)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVb9WF6h0dsn"
      },
      "source": [
        "## Let's start the coding\n",
        "\n",
        "We first download two companion files where we have some useful function and also the main model architecture code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EIZ4Jd-FZo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc267bc0-69cc-4d1f-f8d4-779a5246915a"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/model.py\n",
        "!wget https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/utils.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-29 22:09:57--  https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9935 (9.7K) [text/plain]\n",
            "Saving to: ‘model.py’\n",
            "\n",
            "model.py            100%[===================>]   9.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-29 22:09:57 (60.6 MB/s) - ‘model.py’ saved [9935/9935]\n",
            "\n",
            "--2022-03-29 22:09:57--  https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2602 (2.5K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "utils.py            100%[===================>]   2.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-29 22:09:58 (37.1 MB/s) - ‘utils.py’ saved [2602/2602]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fSb9dkDGOtF"
      },
      "source": [
        "### Let's install tree-hugger and transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG-hxpQ6F-Zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62410c74-9098-4d02-fcf8-0580e87fa0bb"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -U tree-hugger PyYAML"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 4.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 37.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 54.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n",
            "Collecting tree-hugger\n",
            "  Downloading tree_hugger-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Collecting tree-sitter\n",
            "  Downloading tree_sitter-0.20.0.tar.gz (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 6.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygit2\n",
            "  Downloading pygit2-1.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 40.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tree-hugger) (3.6.4)\n",
            "Requirement already satisfied: cffi>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from pygit2->tree-hugger) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from pygit2->tree-hugger) (1.5.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.9.1->pygit2->tree-hugger) (2.21)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (57.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (1.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (8.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (21.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (0.7.1)\n",
            "Building wheels for collected packages: tree-sitter\n",
            "  Building wheel for tree-sitter (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tree-sitter: filename=tree_sitter-0.20.0-cp37-cp37m-linux_x86_64.whl size=304865 sha256=f2f36097eedfe5c6476006a3317a8b40fa89d2a83191666c2485ff9ff70c09f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/70/97/43e08c95ce68348303fd3f71adfa96193db8d9cf60a14163e0\n",
            "Successfully built tree-sitter\n",
            "Installing collected packages: tree-sitter, pygit2, tree-hugger\n",
            "Successfully installed pygit2-1.9.1 tree-hugger-0.10.1 tree-sitter-0.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7I6O9LHZBtt"
      },
      "source": [
        "### And use this command to build the necessary processing libary (tree-hugger related)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bgf9OYFTBgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d58628f-e245-4ddd-93d3-1e5b417a8611"
      },
      "source": [
        "!create_libs -c python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-29 22:10:57,383 INFO:Cloneing python repo from tree-sitter collections\n",
            "2022-03-29 22:11:16,690 INFO:Creating the library my-languages.so at /content\n",
            "2022-03-29 22:11:17,787 INFO:Finished creating library!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ4fxRRo1EJn"
      },
      "source": [
        "**Let's import all necessary modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90hu6R8cFyjd"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from model import Seq2Seq\n",
        "from utils import Example, convert_examples_to_features\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDqbzy7L1Qac"
      },
      "source": [
        "Now that we have everything, let's download the fine-tuned model. Codist has fine tuned this model for your testing purpose 😃"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y6oZ09meZ_q2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCSFPT6xYtkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7c08de-f69e-422c-848d-a93553395568"
      },
      "source": [
        "!wget https://code-summary.s3.amazonaws.com/pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-29 23:09:18--  https://code-summary.s3.amazonaws.com/pytorch_model.bin\n",
            "Resolving code-summary.s3.amazonaws.com (code-summary.s3.amazonaws.com)... 52.217.134.49\n",
            "Connecting to code-summary.s3.amazonaws.com (code-summary.s3.amazonaws.com)|52.217.134.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 706871064 (674M) [application/macbinary]\n",
            "Saving to: ‘pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>] 674.12M  15.8MB/s    in 44s     \n",
            "\n",
            "2022-03-29 23:10:03 (15.3 MB/s) - ‘pytorch_model.bin’ saved [706871064/706871064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC8ol4ii2JOq"
      },
      "source": [
        "## We are defining all the needed functions here. \n",
        "def inference(data, model, tokenizer):\n",
        "    # Calculate bleu\n",
        "    eval_sampler = SequentialSampler(data)\n",
        "    eval_dataloader = DataLoader(data, sampler=eval_sampler, batch_size=len(data))\n",
        "\n",
        "    model.eval()\n",
        "    p = []\n",
        "    for batch in eval_dataloader:\n",
        "        batch = tuple(t.to('cpu') for t in batch)\n",
        "        source_ids, source_mask = batch\n",
        "        with torch.no_grad():\n",
        "            preds = model(source_ids=source_ids, source_mask=source_mask)\n",
        "            for pred in preds:\n",
        "                t = pred[0].cpu().numpy()\n",
        "                t = list(t)\n",
        "                if 0 in t:\n",
        "                    t = t[: t.index(0)]\n",
        "                text = tokenizer.decode(t, clean_up_tokenization_spaces=False)\n",
        "                p.append(text)\n",
        "    return (p, source_ids.shape[-1])\n",
        "\n",
        "\n",
        "def get_features(examples, tokenizer):\n",
        "    features = convert_examples_to_features(\n",
        "        examples, tokenizer, stage=\"test\"\n",
        "    )\n",
        "    all_source_ids = torch.tensor(\n",
        "        [f.source_ids[: 256] for f in features], dtype=torch.long\n",
        "    )\n",
        "    all_source_mask = torch.tensor(\n",
        "        [f.source_mask[: 256] for f in features], dtype=torch.long\n",
        "    )\n",
        "    return TensorDataset(all_source_ids, all_source_mask)\n",
        "\n",
        "\n",
        "def build_model(model_class, config, tokenizer):\n",
        "    encoder = model_class(config=config)\n",
        "    decoder_layer = nn.TransformerDecoderLayer(\n",
        "        d_model=config.hidden_size, nhead=config.num_attention_heads\n",
        "    )\n",
        "    decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
        "    model = Seq2Seq(\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        config=config,\n",
        "        beam_size=10,\n",
        "        max_length=128,\n",
        "        sos_id=tokenizer.cls_token_id,\n",
        "        eos_id=tokenizer.sep_token_id,\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(\n",
        "        torch.load(\n",
        "            \"pytorch_model.bin\",\n",
        "            map_location=torch.device(\"cpu\"),\n",
        "        ),\n",
        "        strict=False,\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LJ-LOTJ2TzB"
      },
      "source": [
        "Now that we have all the needed functions, let's load the baseline model from Hugging Face model hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vQDXXLoMbe6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWFeZDn52a0H"
      },
      "source": [
        "config = RobertaConfig.from_pretrained(\"microsoft/codebert-base\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\n",
        "    \"microsoft/codebert-base\", do_lower_case=False\n",
        ")\n",
        "\n",
        "model = build_model(\n",
        "    model_class=RobertaModel, config=config, tokenizer=tokenizer\n",
        ").to('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp9j7zSa2iX-"
      },
      "source": [
        "Everything is ready to make predictions!! Let's do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cBV950F20bc",
        "outputId": "6b61ac07-bb40-4615-ee73-8d4b92cfbcee"
      },
      "source": [
        "example = [Example(source=\"def add_tensors(t, t1) -> Any:\\n    return t + t1\", target=None)]\n",
        "message, length = inference(get_features(example, tokenizer), model, tokenizer)\n",
        "print(message)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/model.py:219: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  prevK = bestScoresId // numWords\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Add two tensors .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFBsTi-A27ZH"
      },
      "source": [
        "**AMAZING!!**\n",
        "\n",
        "We need to be able to run it on a bunch of files and extract the functions from it and then predict their docstrings. How shall we do it?\n",
        "\n",
        "\n",
        "Codist [tree-hugger](https://github.com/autosoft-dev/tree-hugger) to the rescue!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbRAiCdjdkqE"
      },
      "source": [
        "For the ease of the tutorial we have created a small github example repo with a collection of files. Some of it is coming from Open Source repos and some we created as example files. \n",
        "\n",
        "Let's clone that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sVdIc1idhcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f40533e-7e15-490a-b390-eb15d4fa50e7"
      },
      "source": [
        "!git clone https://github.com/autosoft-dev/example-files.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'example-files'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 0 (delta 0), pack-reused 133\u001b[K\n",
            "Receiving objects: 100% (133/133), 356.91 KiB | 2.23 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHM0NAnCeLaf"
      },
      "source": [
        "We are going to declare a small function that will help us go over each files in a nested directory tree (like the one above we cloned) and get each file at a time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cWsYhNyeIb5"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def check_out_path(target_path: Path):\n",
        "    \"\"\"\"\n",
        "    This function recursively yields all contents of a pathlib.Path object\n",
        "    \"\"\"\n",
        "    yield target_path\n",
        "    for file in target_path.iterdir():\n",
        "        if file.is_dir():\n",
        "            yield from check_out_path(file)\n",
        "        else:\n",
        "            yield file.absolute()\n",
        "\n",
        "\n",
        "def is_python_file(file_path: Path):\n",
        "  \"\"\"\n",
        "  This little function will help us to filter the result and keep only the python files\n",
        "  \"\"\"\n",
        "  return file_path.is_file() and file_path.suffix == \".py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB6OniXlenyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c59c0fd-749e-4fd1-e37d-2414d41340a0"
      },
      "source": [
        "for file_path in check_out_path(Path(\"example-files\")):\n",
        "  if is_python_file(file_path):\n",
        "    print(file_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/example-files/ml_files/ChatterBot/adapters.py\n",
            "/content/example-files/ml_files/ChatterBot/constants.py\n",
            "/content/example-files/ml_files/ChatterBot/chatterbot.py\n",
            "/content/example-files/ml_files/ChatterBot/conversation.py\n",
            "/content/example-files/ml_files/ChatterBot/corpus.py\n",
            "/content/example-files/ml_files/ChatterBot/comparisons.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/apps.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0007_response_created_at.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0004_rename_in_response_to.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0011_blank_extra_data.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0005_statement_created_at.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0012_statement_created_at.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0014_remove_statement_extra_data.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0015_statement_persona.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0013_change_conversations.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0016_statement_stemmed_text.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0006_create_conversation.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0017_tags_unique.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0001_initial.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0010_statement_text.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0003_change_occurrence_default.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0002_statement_extra_data.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0008_update_conversations.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0009_tags.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/migrations/0018_text_max_length.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/admin.py\n",
            "/content/example-files/ml_files/ChatterBot/ext/django_chatterbot/abstract_models.py\n",
            "/content/example-files/ml_files/ChatterBot/exceptions.py\n",
            "/content/example-files/ml_files/_init.py\n",
            "/content/example-files/ml_files/bert/run_pretraining.py\n",
            "/content/example-files/ml_files/bert/run_classifier.py\n",
            "/content/example-files/ml_files/bert/modeling.py\n",
            "/content/example-files/ml_files/bert/modeling_test.py\n",
            "/content/example-files/ml_files/bert/tokenization.py\n",
            "/content/example-files/ml_files/bert/__init__.py\n",
            "/content/example-files/ml_files/bert/run_classifier_with_tfhub.py\n",
            "/content/example-files/ml_files/bert/create_pretraining_data.py\n",
            "/content/example-files/ml_files/bert/optimization_test.py\n",
            "/content/example-files/ml_files/bert/optimization.py\n",
            "/content/example-files/ml_files/bert/tokenization_test.py\n",
            "/content/example-files/ml_files/bert/extract_features.py\n",
            "/content/example-files/ml_files/bert/run_squad.py\n",
            "/content/example-files/ml_files/sklearn/multioutput.py\n",
            "/content/example-files/ml_files/sklearn/kernel_ridge.py\n",
            "/content/example-files/ml_files/sklearn/neural_network/_rbm.py\n",
            "/content/example-files/ml_files/sklearn/neural_network/_base.py\n",
            "/content/example-files/ml_files/sklearn/neural_network/__init__.py\n",
            "/content/example-files/ml_files/sklearn/neural_network/_stochastic_optimizers.py\n",
            "/content/example-files/ml_files/sklearn/neural_network/_multilayer_perceptron.py\n",
            "/content/example-files/ml_files/sklearn/preprocessing/_label.py\n",
            "/content/example-files/ml_files/sklearn/preprocessing/_data.py\n",
            "/content/example-files/ml_files/sklearn/preprocessing/_function_transformer.py\n",
            "/content/example-files/ml_files/sklearn/preprocessing/__init__.py\n",
            "/content/example-files/ml_files/sklearn/preprocessing/setup.py\n",
            "/content/example-files/ml_files/sklearn/preprocessing/_discretization.py\n",
            "/content/example-files/ml_files/sklearn/preprocessing/_encoders.py\n",
            "/content/example-files/ml_files/sklearn/pipeline.py\n",
            "/content/example-files/ml_files/sklearn/semi_supervised/tests/test_self_training.py\n",
            "/content/example-files/ml_files/sklearn/semi_supervised/tests/test_label_propagation.py\n",
            "/content/example-files/ml_files/sklearn/random_projection.py\n",
            "/content/example-files/ml_files/sklearn/discriminant_analysis.py\n",
            "/content/example-files/ml_files/sklearn/dummy.py\n",
            "/content/example-files/ml_files/sklearn/neighbors/_nearest_centroid.py\n",
            "/content/example-files/ml_files/sklearn/neighbors/_classification.py\n",
            "/content/example-files/ml_files/sklearn/neighbors/_unsupervised.py\n",
            "/content/example-files/ml_files/sklearn/neighbors/_regression.py\n",
            "/content/example-files/ml_files/sklearn/neighbors/_kde.py\n",
            "/content/example-files/ml_files/sklearn/neighbors/_base.py\n",
            "/content/example-files/ml_files/sklearn/neighbors/__init__.py\n",
            "/content/example-files/ml_files/sklearn/neighbors/setup.py\n",
            "/content/example-files/ml_files/sklearn/neighbors/_nca.py\n",
            "/content/example-files/ml_files/sklearn/neighbors/_lof.py\n",
            "/content/example-files/ml_files/sklearn/neighbors/_graph.py\n",
            "/content/example-files/ml_files/sklearn/multiclass.py\n",
            "/content/example-files/ml_files/sklearn/kernel_approximation.py\n",
            "/content/example-files/ml_files/sklearn/base.py\n",
            "/content/example-files/ml_files/sklearn/calibration.py\n",
            "/content/example-files/ml_files/sklearn/exceptions.py\n",
            "/content/example-files/ml_files/sklearn/isotonic.py\n",
            "/content/example-files/ml_files/sklearn/naive_bayes.py\n",
            "/content/example-files/api.py\n",
            "/content/example-files/inner_dir/_internal_utils.py\n",
            "/content/example-files/simple_funcs/simple_funcs.py\n",
            "/content/example-files/flask_files/cli.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxp1hwkWgKOQ"
      },
      "source": [
        "We are now ready to use tree-hugger to parse all the needed files and let's do that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTQTIPvFexFo"
      },
      "source": [
        "# We first create our PythonParser object\n",
        "from tree_hugger.core import PythonParser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrlIUSCBgecv"
      },
      "source": [
        "pp = PythonParser(library_loc=\"/content/my-languages.so\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8Eyii31gjk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "decb0a5f-b6af-4324-c13f-60faa8e41ab3"
      },
      "source": [
        "# Let's use the function we defined before to go over all the files.\n",
        "for file_path in check_out_path(Path(\"example-files\")):\n",
        "  if is_python_file(file_path):\n",
        "    # we use one line, super convinient tree-hugger API call to get the needed data\n",
        "    if pp.parse_file(str(file_path)):\n",
        "      temp_cache = []\n",
        "      # The following call returns a dict where each key is a name of a function\n",
        "      # And each value is a tuple, (function_body, function_docstring)\n",
        "      func_and_docstr = pp.get_all_function_bodies(strip_docstr=True)\n",
        "      for func_name, (body, docstr) in func_and_docstr.items():\n",
        "        example = [Example(source=body, target=None)]\n",
        "        message, length = inference(get_features(example, tokenizer), model, tokenizer)\n",
        "        print(func_name, \" \".join(message))\n",
        "      # Let's add the result to the final output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/model.py:219: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  prevK = bestScoresId // numWords\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_file_path Get the file path from the given dotted path .\n",
            "read_corpus Read a corpus .\n",
            "list_corpus_files Return a list of files in a dotted path .\n",
            "load_corpus Yields a list of conversations .\n",
            "model_fn_builder Build a model_fn for TPUEstimator .\n",
            "model_fn Train the BERT model .\n",
            "tpu_scaffold Get a tf . scaffold . scaffold .\n",
            "metric_fn Compute metric function .\n",
            "get_masked_lm_output Create a masked Lm output layer .\n",
            "get_next_sentence_output Get the next sentence output .\n",
            "gather_indexes Gather indices from a tensor .\n",
            "input_fn_builder Builds the input function .\n",
            "input_fn Define input function .\n",
            "_decode_record Decodes a record into int32 .\n",
            "main Main entry point .\n",
            "convert_single_example Convert single example to InputFeatures .\n",
            "file_based_convert_examples_to_features Converts a list of examples into features .\n",
            "create_int_feature Create a tf . Feature .\n",
            "file_based_input_fn_builder Build a TensorFlow input function .\n",
            "_decode_record Decode a record to int32 .\n",
            "input_fn Input function .\n",
            "_truncate_seq_pair Truncates sequences between tokens_a and tokens_b .\n",
            "create_model Create a BERT model .\n",
            "model_fn_builder Build a model_fn for TPUEstimator .\n",
            "model_fn The model function .\n",
            "tpu_scaffold Get a tf . scaffold . scaffold .\n",
            "metric_fn Compute metric function .\n",
            "input_fn_builder Build the input function .\n",
            "convert_examples_to_features Converts a list of examples into a list of features .\n",
            "main Main entry point .\n",
            "gelu  Geluu u .\n",
            "get_activation Get activation function .\n",
            "get_assignment_map_from_checkpoint Builds a mapping of variables to assignment_map .\n",
            "dropout Dropout tf .\n",
            "layer_norm Layer normalization layer .\n",
            "layer_norm_and_dropout Layer normalization layer .\n",
            "create_initializer Create an initializer for an initializer .\n",
            "embedding_lookup Lookup embedding table .\n",
            "embedding_postprocessor Embedding postprocessor .\n",
            "create_attention_mask_from_input_mask Create attention mask from input_tensor .\n",
            "attention_layer The attention layer .\n",
            "transpose_for_scores Transpose tensors for scores .\n",
            "transformer_model Transformer transformer .\n",
            "get_shape_list Get the shape of a tensor .\n",
            "reshape_to_matrix Reshapes a tensor to a matrix .\n",
            "reshape_from_matrix Reshape a tensor from orig_shape .\n",
            "assert_rank Assert that tensor has the expected rank .\n",
            "validate_case_matches_checkpoint Validates that the casing matches the given checkpoint .\n",
            "convert_to_unicode Convert a string to unicode .\n",
            "printable_text Return a string representation of the given input .\n",
            "load_vocab Loads vocabulary from file .\n",
            "convert_by_vocab Convert a list of strings to a list of strings .\n",
            "whitespace_tokenize Split text into tokens .\n",
            "_is_whitespace Check if character is a whitespace character .\n",
            "_is_control Check if character is a control character .\n",
            "_is_punctuation Check if character is punctuation .\n",
            "create_model Create a BERT model .\n",
            "model_fn_builder Build a model_fn for TPUEstimator .\n",
            "model_fn Create a tf . model .\n",
            "metric_fn Calculate metric function .\n",
            "create_tokenizer_from_hub_module Create a tokenizer from a BERT hub module .\n",
            "main Main function .\n",
            "write_instance_to_example_files Write examples to example files .\n",
            "create_int_feature Create a tf . Feature .\n",
            "create_float_feature Create a tf . Feature .\n",
            "create_training_instances Create TrainingInstance .\n",
            "create_instances_from_document Create instances from a document .\n",
            "create_masked_lm_predictions Create masked Lm predictions .\n",
            "truncate_seq_pair Truncate a sequence pair .\n",
            "main Create TrainingInstance .\n",
            "create_optimizer Creates an optimizer .\n",
            "input_fn_builder Generate input function .\n",
            "input_fn Input function .\n",
            "model_fn_builder Build a model_fn for TPUEstimator .\n",
            "model_fn Train a BERT model .\n",
            "tpu_scaffold Returns a tf . scaffold for TPU .\n",
            "convert_examples_to_features Converts a list of tokens into tokens .\n",
            "_truncate_seq_pair Truncates sequences between tokens_a and tokens_b .\n",
            "read_examples Read examples from a file .\n",
            "main Main function .\n",
            "read_squad_examples Reads Squadoc examples from a file .\n",
            "is_whitespace Check if character is a whitespace character .\n",
            "convert_examples_to_features Converts a list of tokens into features .\n",
            "_improve_answer_span Improve the answer span .\n",
            "_check_is_max_context Helper function for _check_max_context .\n",
            "create_model Create a BERT model .\n",
            "model_fn_builder Build a model_fn for TPUEstimator .\n",
            "model_fn Create a tf . model .\n",
            "tpu_scaffold Get a tf . scaffold . scaffold .\n",
            "compute_loss Compute loss .\n",
            "input_fn_builder Builds a TensorFlow input function .\n",
            "_decode_record Decode a record to int32 .\n",
            "input_fn Input function .\n",
            "write_predictions Write predictions to nbest prediction file .\n",
            "get_final_text Calculate the final output of the prediction .\n",
            "_strip_spaces Strip whitespace from a string .\n",
            "_get_best_indexes Returns a list of the best logits for the given logits .\n",
            "_compute_softmax Compute softmax of given scores .\n",
            "create_int_feature Create a tf . Feature .\n",
            "validate_flags_or_throw Validate flags and raise ValueError .\n",
            "main Main function .\n",
            "append_feature Append a new feature .\n",
            "_fit_estimator Helper function to fit estimator\n",
            "_partial_fit_estimator Helper function to fit estimator\n",
            "inplace_identity Inplace identity .\n",
            "inplace_logistic Compute logistic logistic .\n",
            "inplace_tanh Inplace inplace\n",
            "inplace_relu Inplace inplace .\n",
            "inplace_softmax Inplace softmax .\n",
            "inplace_identity_derivative Inplace identity derivative\n",
            "inplace_logistic_derivative Inplace logistic derivative .\n",
            "inplace_tanh_derivative Inplace inplace_tanh derivative\n",
            "inplace_relu_derivative Derivative of the inplace matrix\n",
            "squared_loss Squared loss .\n",
            "log_loss Log loss function\n",
            "binary_log_loss Binary log loss .\n",
            "_pack Pack the coefficients and intercepts .\n",
            "label_binarize Babelize y .\n",
            "_inverse_binarize_multiclass Inverse of _binarize_multiclass .\n",
            "_inverse_binarize_thresholding Perform inverse binarization on numpy arrays .\n",
            "_handle_zeros_in_scale Check if scale is a scalar .\n",
            "scale Scale X along an axis .\n",
            "minmax_scale Minimum scaling function .\n",
            "maxabs_scale MaxAbsolute scale function .\n",
            "robust_scale Robust scaling .\n",
            "normalize Normalize an array of vectors .\n",
            "binarize Binarize a sparse matrix .\n",
            "add_dummy_feature Add dummy feature .\n",
            "quantile_transform Transform a quantile transformer .\n",
            "_neg_log_likelihood Negative log - likelihood .\n",
            "power_transform Power transform .\n",
            "_identity Identify X .\n",
            "configuration Create a configuration object .\n",
            "_get_name Get name for estimator .\n",
            "_name_estimators Generate name for estimators .\n",
            "make_pipeline Create a pipeline for pipeline estimators .\n",
            "_transform_one Apply one transformer to X .\n",
            "_fit_transform_one Helper function for fit_transform .\n",
            "_fit_one Wrapper for fit_one .\n",
            "make_union Create a FeatureUnion .\n",
            "test_missing_predict_proba Test the missing predict probabilities .\n",
            "test_none_classifier Test that test is None .\n",
            "test_invalid_params Test for invalid parameters .\n",
            "test_invalid_params_selection_crit Runs test on validation criterion .\n",
            "test_warns_k_best Raises an AssertionError if self . k_best_classifier is too large .\n",
            "test_classification Classifier test .\n",
            "test_k_best Test the K nearest neighbors .\n",
            "test_sanity_classification Test the sanity test .\n",
            "test_none_iter Runs test .\n",
            "test_zero_iterations Runs test on test_iter .\n",
            "test_prefitted_throws_error Test if self - trained classifier is not fitted .\n",
            "test_labeled_iter Test that the label is less than max_iter .\n",
            "test_no_unlabeled Assert that no unlabeled samples are labeled .\n",
            "test_early_stopping Test the early stopping method .\n",
            "test_strings_dtype Test the numpy classifier .\n",
            "test_verbose Runs test on the given capsyset test .\n",
            "test_verbose_k_best Runs test on the KNeighbors classifier .\n",
            "test_k_best_selects_best Testing test for k_best .\n",
            "test_fit_transduction Test the fitting of the Estimator .\n",
            "test_distribution Test the distribution of estimators .\n",
            "test_predict Test accuracy of estimators .\n",
            "test_predict_proba Test for test_proba .\n",
            "test_label_spreading_closed_form Run test on labelspreading .\n",
            "test_label_propagation_closed_form Test label propagation .\n",
            "test_valid_alpha Test label - propagation .\n",
            "test_convergence_speed Test convergence speed .\n",
            "test_convergence_warning Test if converging warning is valid .\n",
            "test_label_propagation_non_zero_normalizer Test label propagation .\n",
            "test_predict_sparse_callable_kernel Testing function for test_callable .\n",
            "topk_rbf Compute top k nearest neighbors .\n",
            "johnson_lindenstrauss_min_dim Calculate the minimum dimensionality of an array of samples .\n",
            "_check_density Check if density is valid .\n",
            "_check_input_size Ensure input size is positive .\n",
            "_gaussian_random_matrix Generate Gaussian random components .\n",
            "_sparse_random_matrix Generate a random sparse matrix .\n",
            "_cov Compute covariance matrix\n",
            "_class_means Compute the class - wise mean .\n",
            "_class_cov Compute the covariance matrix .\n",
            "_check_weights Check if weights are uniform .\n",
            "_get_weights Helper function for _get_weights .\n",
            "_is_sorted_by_data Return True if graph is sorted by line change .\n",
            "_check_precomputed Check sparse matrix for sparse matrix .\n",
            "_kneighbors_from_graph Calculate k nearest neighbors from a graph .\n",
            "extract Extract n_samples from a matrix .\n",
            "_radius_neighbors_from_graph Get the neighbors of a graph .\n",
            "_tree_query_parallel_helper Parse tree query helper .\n",
            "_tree_query_radius_parallel_helper Helper function to wrap tree query around tree .\n",
            "configuration Create a configuration object .\n",
            "_check_params Check for metric_params .\n",
            "_query_include_self Add self to self . _self to self . _self .\n",
            "kneighbors_graph Compute the KNeighbors Networking Graph\n",
            "radius_neighbors_graph Compute a network - neighbors graph for a given radius .\n",
            "_fit_binary Fit the estimator .\n",
            "_partial_fit_binary Perform a partial fit on the estimator .\n",
            "_predict_binary Predict the probability of an estimator .\n",
            "_check_estimator Checks that the base estimator is valid .\n",
            "_fit_ovo_binary Fit an objective function to the model .\n",
            "_partial_fit_ovo_binary Compute the partial fit of an objective function .\n",
            "clone Clone an estimator\n",
            "_pprint Pretty print the parameters .\n",
            "is_classifier Returns True if estimator is a classifier .\n",
            "is_regressor Returns True if estimator is a regressor .\n",
            "is_outlier_detector Returns True if estimator is an outlier detector .\n",
            "_is_pairwise Check if estimator is valid .\n",
            "_fit_classifier_calibrator_pair Fit classifier and calibrator .\n",
            "_get_prediction_method Get prediction method .\n",
            "_compute_predictions Compute predictions for prediction .\n",
            "_fit_calibrator Calculate calibration .\n",
            "_sigmoid_calibration Sigmoid calibration .\n",
            "objective The objective function .\n",
            "grad Gradient of gradient descent .\n",
            "calibration_curve Calculate calibration curve .\n",
            "check_increasing Check for increasing rho .\n",
            "isotonic_regression Compute isotonic Regression .\n",
            "_update_cat_count_dims Helper function to update cat_count_dims\n",
            "_update_cat_count update category counts\n",
            "request Perform a HTTP request .\n",
            "get Send a GET request .\n",
            "options HTTP OPTIONS request .\n",
            "head Send a HEAD request .\n",
            "post Perform a POST request\n",
            "put HTTP PUT request .\n",
            "patch Perform an HTTP PATCH request .\n",
            "delete Perform a DELETE request .\n",
            "to_native_string Convert string to native string .\n",
            "unicode_is_ascii Check if unicode is ASCII .\n",
            "add Add two vectors .\n",
            "check_even_numbers_in_a_list Checks that all numbers in a list are equal .\n",
            "open_file open a file\n",
            "add_tensors Add two tensors .\n",
            "find_best_app Find the best Flask application in a module .\n",
            "call_factory Call app factory .\n",
            "_called_with_wrong_args Check if a function has wrong arguments .\n",
            "find_app_by_string Find application by name or function name .\n",
            "prepare_import Prepare a python import .\n",
            "locate_app Locate a Flask application .\n",
            "get_version Print current version\n",
            "_load_app Load the lock .\n",
            "with_appcontext A decorator that adds a click context to a function .\n",
            "decorator Wrapper for click . Command .\n",
            "_path_is_ancestor Check if path is an ancestor of other .\n",
            "load_dotenv Load . env file .\n",
            "show_server_banner Show the server banner .\n",
            "_validate_key Ensure key is valid .\n",
            "run_command Run a werkzeug command .\n",
            "shell_command Create a shell command .\n",
            "routes_command Executor for globus routes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM38KwJ5mzH0"
      },
      "source": [
        "With this code, you can very easily create a dataset out of your own code files and then test the baseline models against it. \n",
        "\n",
        "\n",
        "That was easy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYpSBwH11Shw"
      },
      "source": [
        " As said earlier, codist just relased `docly` we use similar parsing and modelling methods to generate docstring (with arguments and several other things that is not present in this baseline model). If you are interested, please have a look [here](https://codist-ai.com/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use the function we defined before to go over all the files.\n",
        "counter = 0\n",
        "for file_path in check_out_path(Path(\"example-files\")):\n",
        "  if counter > 30:\n",
        "    break\n",
        "  if is_python_file(file_path):\n",
        "    # we use one line, super convinient tree-hugger API call to get the needed data\n",
        "    if pp.parse_file(str(file_path)):\n",
        "      counter += 1\n",
        "      temp_cache = []\n",
        "      # The following call returns a dict where each key is a name of a function\n",
        "      # And each value is a tuple, (function_body, function_docstring)\n",
        "      func_and_docstr = pp.get_all_function_bodies(strip_docstr=True)\n",
        "      for func_name, (body, docstr) in func_and_docstr.items():\n",
        "        example = [Example(source=body, target=None)]\n",
        "        message, length = inference(get_features(example, tokenizer), model, tokenizer)\n",
        "        if docstr != \"\":\n",
        "          print(func_name,\n",
        "                \"\\nSummary: [\" + \" \".join(message) + \"]\",\n",
        "                \"\\nDoc string: [\" + docstr + \"]\")\n",
        "      # Let's add the result to the final output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFwDwxbiLbuk",
        "outputId": "760f76b7-bcc6-4868-f337-eeb4a55f4b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/model.py:219: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  prevK = bestScoresId // numWords\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_file_path \n",
            "Summary: [Get the file path from the given dotted path .] \n",
            "Doc string: [\"\"\"\n",
            "    Reads a dotted file path and returns the file path.\n",
            "    \"\"\"]\n",
            "read_corpus \n",
            "Summary: [Read a corpus .] \n",
            "Doc string: [\"\"\"\n",
            "    Read and return the data from a corpus json file.\n",
            "    \"\"\"]\n",
            "list_corpus_files \n",
            "Summary: [Return a list of files in a dotted path .] \n",
            "Doc string: [\"\"\"\n",
            "    Return a list of file paths to each data file in the specified corpus.\n",
            "    \"\"\"]\n",
            "load_corpus \n",
            "Summary: [Yields a list of conversations .] \n",
            "Doc string: [\"\"\"\n",
            "    Return the data contained within a specified corpus.\n",
            "    \"\"\"]\n",
            "model_fn_builder \n",
            "Summary: [Build a model_fn for TPUEstimator .] \n",
            "Doc string: [\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"]\n",
            "model_fn \n",
            "Summary: [Train the BERT model .] \n",
            "Doc string: [\"\"\"The `model_fn` for TPUEstimator.\"\"\"]\n",
            "metric_fn \n",
            "Summary: [Compute metric function .] \n",
            "Doc string: [\"\"\"Computes the loss and accuracy of the model.\"\"\"]\n",
            "get_masked_lm_output \n",
            "Summary: [Create a masked Lm output layer .] \n",
            "Doc string: [\"\"\"Get loss and log probs for the masked LM.\"\"\"]\n",
            "get_next_sentence_output \n",
            "Summary: [Get the next sentence output .] \n",
            "Doc string: [\"\"\"Get loss and log probs for the next sentence prediction.\"\"\"]\n",
            "gather_indexes \n",
            "Summary: [Gather indices from a tensor .] \n",
            "Doc string: [\"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"]\n",
            "input_fn_builder \n",
            "Summary: [Builds the input function .] \n",
            "Doc string: [\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"]\n",
            "input_fn \n",
            "Summary: [Define input function .] \n",
            "Doc string: [\"\"\"The actual input function.\"\"\"]\n",
            "_decode_record \n",
            "Summary: [Decodes a record into int32 .] \n",
            "Doc string: [\"\"\"Decodes a record to a TensorFlow example.\"\"\"]\n",
            "convert_single_example \n",
            "Summary: [Convert single example to InputFeatures .] \n",
            "Doc string: [\"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"]\n",
            "file_based_convert_examples_to_features \n",
            "Summary: [Converts a list of examples into features .] \n",
            "Doc string: [\"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"]\n",
            "file_based_input_fn_builder \n",
            "Summary: [Build a TensorFlow input function .] \n",
            "Doc string: [\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"]\n",
            "_decode_record \n",
            "Summary: [Decode a record to int32 .] \n",
            "Doc string: [\"\"\"Decodes a record to a TensorFlow example.\"\"\"]\n",
            "input_fn \n",
            "Summary: [Input function .] \n",
            "Doc string: [\"\"\"The actual input function.\"\"\"]\n",
            "_truncate_seq_pair \n",
            "Summary: [Truncates sequences between tokens_a and tokens_b .] \n",
            "Doc string: [\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"]\n",
            "create_model \n",
            "Summary: [Create a BERT model .] \n",
            "Doc string: [\"\"\"Creates a classification model.\"\"\"]\n",
            "model_fn_builder \n",
            "Summary: [Build a model_fn for TPUEstimator .] \n",
            "Doc string: [\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"]\n",
            "model_fn \n",
            "Summary: [The model function .] \n",
            "Doc string: [\"\"\"The `model_fn` for TPUEstimator.\"\"\"]\n",
            "input_fn_builder \n",
            "Summary: [Build the input function .] \n",
            "Doc string: [\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"]\n",
            "convert_examples_to_features \n",
            "Summary: [Converts a list of examples into a list of features .] \n",
            "Doc string: [\"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"]\n"
          ]
        }
      ]
    }
  ]
}