{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distill coberta",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference: \n",
        "- code: https://gist.github.com/remi-or/4814577c59f4f38fcc89729ce4ba21e6\n",
        "- tutorial: https://towardsdatascience.com/distillation-of-bert-like-models-the-code-73c31e8c2b0a"
      ],
      "metadata": {
        "id": "VgqMDrCwSl6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOZDzIfT7Y56",
        "outputId": "8fd0e59d-1659-48db-9c84-df18f8c0814a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHonp2RXTFNI",
        "outputId": "1b171509-f119-4b25-a871-a537f526fe9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install pytorch-cpu torchvision-cpu -c pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCCyhEprV3Ir",
        "outputId": "23e4b758-1180-4921-830f-32cd75c7e34e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DHIblM0p4JP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "from typing import Tuple\n",
        "import torch\n",
        "from torch.nn import Module\n",
        "from torch import Tensor\n",
        "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel, RobertaConfig, RobertaModel, RobertaEncoder\n",
        "from torch.nn import CrossEntropyLoss, CosineEmbeddingLoss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta = AutoModelForMaskedLM.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "id": "-PyMSlfb9dTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.roberta.modeling_roberta import RobertaEncoder, RobertaModel\n",
        "from torch.nn import Module\n",
        "\n",
        "def distill_roberta_weights(\n",
        "    teacher : Module,\n",
        "    student : Module,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Recursively copies the weights of the (teacher) to the (student).\n",
        "    This function is meant to be first called on a RobertaFor... model, but is then called on every children of that model recursively.\n",
        "    The only part that's not fully copied is the encoder, of which only half is copied.\n",
        "    \"\"\"\n",
        "    # If the part is an entire RoBERTa model or a RobertaFor..., unpack and iterate\n",
        "    if isinstance(teacher, RobertaModel) or type(teacher).__name__.startswith('RobertaFor'):\n",
        "        for teacher_part, student_part in zip(teacher.children(), student.children()):\n",
        "            distill_roberta_weights(teacher_part, student_part)\n",
        "    # Else if the part is an encoder, copy one out of every layer\n",
        "    elif isinstance(teacher, RobertaEncoder):\n",
        "            teacher_encoding_layers = [layer for layer in next(teacher.children())]\n",
        "            student_encoding_layers = [layer for layer in next(student.children())]\n",
        "            for i in range(len(student_encoding_layers)):\n",
        "                student_encoding_layers[i].load_state_dict(teacher_encoding_layers[2*i].state_dict())\n",
        "    # Else the part is a head or something else, copy the state_dict\n",
        "    else:\n",
        "        student.load_state_dict(teacher.state_dict())"
      ],
      "metadata": {
        "id": "Vcmp4GONAZmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distill_roberta(\n",
        "    teacher_model : RobertaPreTrainedModel,\n",
        ") -> RobertaPreTrainedModel:\n",
        "    \"\"\"\n",
        "    Distilates a RoBERTa (teacher_model) like would DistilBERT for a BERT model.\n",
        "    The student model has the same configuration, except for the number of hidden layers, which is // by 2.\n",
        "    The student layers are initilized by copying one out of two layers of the teacher, starting with layer 0.\n",
        "    The head of the teacher is also copied.\n",
        "    \"\"\"\n",
        "    # Get teacher configuration as a dictionnary\n",
        "    configuration = teacher_model.config.to_dict()\n",
        "    # Half the number of hidden layer\n",
        "    configuration['num_hidden_layers'] //= 2\n",
        "    # Convert the dictionnary to the student configuration\n",
        "    configuration = RobertaConfig.from_dict(configuration)\n",
        "    # Create uninitialized student model\n",
        "    student_model = type(teacher_model)(configuration)\n",
        "    # Initialize the student's weights\n",
        "    distill_roberta_weights(teacher=teacher_model, student=student_model)\n",
        "    # Return the student model\n",
        "    return student_model"
      ],
      "metadata": {
        "id": "7LiAMS0z_dEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "\n",
        "# Init\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "codebert_model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "# teacher\n",
        "codebert_model.config"
      ],
      "metadata": {
        "id": "ZRfofDyEcBWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0da2c129-20d2-41a0-ddad-1b5506ed4a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"_name_or_path\": \"microsoft/codebert-base\",\n",
              "  \"architectures\": [\n",
              "    \"RobertaModel\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"max_position_embeddings\": 514,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"output_past\": true,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.18.0\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50265\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STUDENT\")\n",
        "student = distill_roberta(codebert_model)\n",
        "print(student.config)\n",
        "print(\"TEACHER\")\n",
        "codebert_model.config\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOMPX_lxcWFG",
        "outputId": "7503b21b-16ef-4eec-faaf-976a26181205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STUDENT\n",
            "RobertaConfig {\n",
            "  \"_name_or_path\": \"microsoft/codebert-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "TEACHER\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"_name_or_path\": \"microsoft/codebert-base\",\n",
              "  \"architectures\": [\n",
              "    \"RobertaModel\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"max_position_embeddings\": 514,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"output_past\": true,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.18.0\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50265\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Distllator class\n",
        "class Distillator(Module):\n",
        "\n",
        "    \"\"\"\n",
        "    A class to distillate a BERT-like model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "        teacher_model : RobertaPreTrainedModel,\n",
        "        temperature : float = 1.0,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initiates the Distillator with the (teacher_model) to distillate from.\n",
        "        \"\"\"\n",
        "        super(Distillator, self).__init__()\n",
        "        self.teacher = teacher_model\n",
        "        self.student = distill_roberta(teacher_model)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    @property\n",
        "    def temperature(self) -> float:\n",
        "        \"\"\"\n",
        "        The temperature used for training can change, but for inference it is always 1.\n",
        "        \"\"\"\n",
        "        return self._temperature if self.training else 1\n",
        "\n",
        "    @temperature.setter\n",
        "    def temperature(self,\n",
        "        value : float,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        The temperature must always be above 1. Otherwise, an error is raised.\n",
        "        \"\"\"\n",
        "        if value < 1:\n",
        "            raise(ValueError(f\"Temperature must be above 1, it cannot be {value}\"))\n",
        "        self._temperature = value\n",
        "\n",
        "    def get_logits(self,\n",
        "        input_ids : Tensor,\n",
        "        attention_mask : Tensor,\n",
        "        from_teacher : bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Given a couple of (input_ids) and (attention_mask), returns the logits corresponding to the prediction.\n",
        "        The logits come from the student unless (from_teacher) is set to True, then it's from the teacher.\n",
        "        \"\"\"\n",
        "        if from_teacher:\n",
        "            return self.teacher.classifier(self.roberta(input_ids, attention_mask)[0])\n",
        "        return self.student.classifier(self.student(input_ids, attention_mask)[0])\n",
        "\n",
        "    def forward(self,\n",
        "        input_ids : Tensor,\n",
        "        attention_mask : Tensor,\n",
        "        labels : Tensor,\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Given a couple of (input_ids) and (attention_mask), returns the logits corresponding to the prediction.\n",
        "        Also takes in the (labels) associated to the inputs.\n",
        "        Returns the student probability distibution with temperature 1 and the loss.\n",
        "        \"\"\"\n",
        "        student_logits = self.get_logits(input_ids, attention_mask, False)\n",
        "        teacher_logits = self.get_logits(input_ids, attention_mask, True)\n",
        "        return student_logits.softmax(1), self.loss(teacher_logits, student_logits, labels)\n",
        "\n",
        "\n",
        "    def loss(self,\n",
        "        teacher_logits : Tensor,\n",
        "        student_logits : Tensor,\n",
        "        labels : Tensor,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        The distillation loss for distilating a BERT-like model.\n",
        "        The loss takes the (teacher_logits), (student_logits) and (labels) for various losses.\n",
        "        \"\"\"\n",
        "        # Temperature and sotfmax\n",
        "        student_logits, teacher_logits = (student_logits / self.temperature).softmax(1), (teacher_logits / self.temperature).softmax(1)\n",
        "        # Classification loss (problem-specific loss)\n",
        "        loss = CrossEntropyLoss()(student_logits, labels)\n",
        "        # CrossEntropy teacher-student loss\n",
        "        loss = loss + CrossEntropyLoss()(student_logits, teacher_logits)\n",
        "        # Cosine loss\n",
        "        loss = loss + CosineEmbeddingLoss()(teacher_logits, student_logits, torch.ones(teacher_logits.size()[0]))\n",
        "        # Average the loss and return it\n",
        "        loss = loss / 3\n",
        "        return loss"
      ],
      "metadata": {
        "id": "yhseNmEPStfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dis = Distillator(roberta)"
      ],
      "metadata": {
        "id": "g_3k9vvpStWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dis.student"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtBhB7MJYXXa",
        "outputId": "2cfc65f6-b678-48c6-8d11-20c1c0019e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForMaskedLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following sections, I will try to implement CodeBERT for comment generation and compare between baseline model and condensed model "
      ],
      "metadata": {
        "id": "CssfImgelopm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
        "# Import generic wrappers\n",
        "from transformers import AutoModel, AutoTokenizer \n",
        "\n",
        "\n",
        "# Define the model repo\n",
        "model_name = \"huggingface/CodeBERTa-small-v1\" \n",
        "\n",
        "\n",
        "# Download pytorch model\n",
        "model_small = AutoModel.from_pretrained(model_name)\n",
        "tokenizer_small = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Transform input tokens \n",
        "inputs_small = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
        "\n",
        "# Model apply\n",
        "# outputs_small = model(**inputs_small)\n",
        "# tokenizer_small = AutoTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\", do_lower_case = False)\n",
        "\n",
        "# model_smallBERT = AutoModelForMaskedLM.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
        "\n",
        "# config_small = RobertaConfig.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
        "print(config_small)\n",
        "print(config_CodeBERT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZNgTYk_jbBf",
        "outputId": "17a9b26f-46ff-4fea-95d8-9db6c5e9422b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/model.py\n",
        "!wget https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_7C49shjqGj",
        "outputId": "59041978-1a11-423e-d2cd-3577fe3326bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-12 22:08:25--  https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9935 (9.7K) [text/plain]\n",
            "Saving to: ‘model.py.2’\n",
            "\n",
            "\rmodel.py.2            0%[                    ]       0  --.-KB/s               \rmodel.py.2          100%[===================>]   9.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-12 22:08:25 (43.7 MB/s) - ‘model.py.2’ saved [9935/9935]\n",
            "\n",
            "--2022-04-12 22:08:26--  https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2602 (2.5K) [text/plain]\n",
            "Saving to: ‘utils.py.2’\n",
            "\n",
            "utils.py.2          100%[===================>]   2.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-04-12 22:08:26 (32.5 MB/s) - ‘utils.py.2’ saved [2602/2602]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tree-hugger PyYAML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBx3KUz_kaSe",
        "outputId": "eb165e9c-5d70-4800-dda2-9aa0dee61272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tree-hugger in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Requirement already satisfied: pygit2 in /usr/local/lib/python3.7/dist-packages (from tree-hugger) (1.9.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tree-hugger) (3.6.4)\n",
            "Requirement already satisfied: tree-sitter in /usr/local/lib/python3.7/dist-packages (from tree-hugger) (0.20.0)\n",
            "Requirement already satisfied: cffi>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from pygit2->tree-hugger) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from pygit2->tree-hugger) (1.5.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.9.1->pygit2->tree-hugger) (2.21)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (8.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (57.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (1.11.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (1.15.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (21.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!create_libs -c python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NBxsckekd9N",
        "outputId": "fef4967e-2dbd-487d-ae35-930d5514ac7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-04-12 22:08:30,845 INFO:Cloneing python repo from tree-sitter collections\n",
            "2022-04-12 22:08:49,757 INFO:Creating the library my-languages.so at /content\n",
            "2022-04-12 22:08:50,784 INFO:Finished creating library!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://code-summary.s3.amazonaws.com/pytorch_model.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaychW8RrZ2I",
        "outputId": "9071448d-c343-40dc-9017-26e499407541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-12 22:08:51--  https://code-summary.s3.amazonaws.com/pytorch_model.bin\n",
            "Resolving code-summary.s3.amazonaws.com (code-summary.s3.amazonaws.com)... 52.217.139.73\n",
            "Connecting to code-summary.s3.amazonaws.com (code-summary.s3.amazonaws.com)|52.217.139.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 706871064 (674M) [application/macbinary]\n",
            "Saving to: ‘pytorch_model.bin.1’\n",
            "\n",
            "pytorch_model.bin.1 100%[===================>] 674.12M  40.7MB/s    in 15s     \n",
            "\n",
            "2022-04-12 22:09:06 (43.5 MB/s) - ‘pytorch_model.bin.1’ saved [706871064/706871064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from model import Seq2Seq\n",
        "from utils import Example, convert_examples_to_features\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForMaskedLM \n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler"
      ],
      "metadata": {
        "id": "m6zQ4PefkkS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We are defining all the needed functions here. \n",
        "def inference(data, model, tokenizer):\n",
        "    # Calculate bleu\n",
        "    eval_sampler = SequentialSampler(data)\n",
        "    eval_dataloader = DataLoader(data, sampler=eval_sampler, batch_size=len(data))\n",
        "\n",
        "    model.eval()\n",
        "    p = []\n",
        "    for batch in eval_dataloader:\n",
        "        batch = tuple(t.to('cpu') for t in batch)\n",
        "        source_ids, source_mask = batch\n",
        "        with torch.no_grad():\n",
        "            preds = model(source_ids=source_ids, source_mask=source_mask)\n",
        "            for pred in preds:\n",
        "                t = pred[0].cpu().numpy()\n",
        "                t = list(t)\n",
        "                if 0 in t:\n",
        "                    t = t[: t.index(0)]\n",
        "                text = tokenizer.decode(t, clean_up_tokenization_spaces=False)\n",
        "                p.append(text)\n",
        "    return (p, source_ids.shape[-1])\n",
        "\n",
        "\n",
        "def get_features(examples, tokenizer):\n",
        "    features = convert_examples_to_features(\n",
        "        examples, tokenizer, stage=\"test\"\n",
        "    )\n",
        "    all_source_ids = torch.tensor(\n",
        "        [f.source_ids[: 256] for f in features], dtype=torch.long\n",
        "    )\n",
        "    all_source_mask = torch.tensor(\n",
        "        [f.source_mask[: 256] for f in features], dtype=torch.long\n",
        "    )\n",
        "    return TensorDataset(all_source_ids, all_source_mask)\n",
        "\n",
        "\n",
        "#Not sure if we need all the stuff below\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "def build_model(model_class, config, tokenizer):\n",
        "    encoder = model_class(config=config)\n",
        "    decoder_layer = nn.TransformerDecoderLayer(\n",
        "        d_model=config.hidden_size, nhead=config.num_attention_heads\n",
        "    )\n",
        "    decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
        "    model = Seq2Seq(\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        config=config,\n",
        "        beam_size=10,\n",
        "        max_length=128,\n",
        "        sos_id=tokenizer.cls_token_id,\n",
        "        eos_id=tokenizer.sep_token_id,\n",
        "    )\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n",
        "    model.load_state_dict(\n",
        "        torch.load(\n",
        "            \"pytorch_model.bin\",\n",
        "            map_location=torch.device(\"cpu\"),\n",
        "        ),\n",
        "        strict=False,\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "bmTfyIs-l94T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_CodeBERT = RobertaConfig.from_pretrained(\"microsoft/codebert-base\")\n",
        "tokenizer_CodeBERT = RobertaTokenizer.from_pretrained(\n",
        "    \"microsoft/codebert-base\", do_lower_case=False\n",
        ")\n",
        "\n",
        "model = build_model(\n",
        "    model_class=RobertaModel, config=config_CodeBERT, tokenizer=tokenizer_CodeBERT\n",
        ").to('cpu')"
      ],
      "metadata": {
        "id": "RI6jJDXtl91u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RobertaForMaskedLM.add_module()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "TOVIOMmxBv9E",
        "outputId": "fcad1e99-373c-4db6-9faa-25779cabf95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-29f6dec2185a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRobertaEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRobertaEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRobertaForMaskedLM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RobertaEmbeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_small = build_model(\n",
        "    model_class=RobertaForMaskedLM, config=config_small, tokenizer=tokenizer_small\n",
        ").to('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "2FUhHWilrqeo",
        "outputId": "8f10a963-f383-4849-c147-8649fa818e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-7ca4056dcd24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_small = build_model(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_small\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m ).to('cpu')\n",
            "\u001b[0;32m<ipython-input-78-b64f96b9e4df>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(model_class, config, tokenizer)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     decoder_layer = nn.TransformerDecoderLayer(\n\u001b[1;32m     44\u001b[0m         \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         raise EnvironmentError(\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;34mf\"{self.__class__.__name__} is designed to be instantiated \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;34mf\"using the `{self.__class__.__name__}.from_pretrained(pretrained_model_name_or_path)` or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;34mf\"`{self.__class__.__name__}.from_config(config)` methods.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: AutoModelForMaskedLM is designed to be instantiated using the `AutoModelForMaskedLM.from_pretrained(pretrained_model_name_or_path)` or `AutoModelForMaskedLM.from_config(config)` methods."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = [Example(source=\"def add_tensors(t, t1) -> Any:\\n    return t + t1\", target=None)]\n",
        "message, length = inference(get_features(example, tokenizer), model, tokenizer)\n",
        "print(message)"
      ],
      "metadata": {
        "id": "WD8HOglll9vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NznP_lIsl9rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZOE6A6kfl9np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "66DlxJXvl9iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6VG_uNncl9Yn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}